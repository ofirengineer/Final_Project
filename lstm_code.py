# -*- coding: utf-8 -*-
"""LSTM_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W0iDZhAjjZ8E6xxMSXHLRklbtaZRaRjZ
"""

# !pip install neuralforecast==3.0.1 pandas matplotlib scikit-learn

import pandas as pd
import numpy as np
from neuralforecast import NeuralForecast
from neuralforecast.models import LSTM
from neuralforecast.losses.pytorch import MAE
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
import os
import scipy.stats # New library for statistics

# --- 1. Define treatment names and corresponding full filenames ---
# The full filename representing each treatment
treatment_full_filenames = {
    "TREATMENT_1": "DG001100624CHR1B03U2OSDOX0NNN0NNN0NNN0NNN0WH00",
    "TREATMENT_2": "DG001100624CHR1B06U2OSDOX0HGF7NNN0NNN0NNN0WH00"
}

# Define global parameters
HORIZON = 10 # Number of last hours for forecasting and comparison
MIN_TOTAL_HOURS_FOR_FORECAST = 30 # Minimum total data hours for a cell to be included in the forecast (e.g., 20 training + 10 forecasting)
LSTM_INPUT_SIZE = 24 # Size of the historical context for LSTM (can be less if not enough data)

# List to collect all metric tables from all treatments and variables
all_consolidated_metrics_dfs = []

# --- 2. Main loop over each treatment ---
for treatment_alias, specific_filename_for_treatment in treatment_full_filenames.items():
    print(f"\n--- Starting analysis for treatment: {treatment_alias} ---")

    # Load the full CSV file each time to ensure a clean start for each treatment
    print("Step 1: Loading data and preprocessing...")
    try:
        df = pd.read_csv('Combined_SummaryTable.csv')
    except FileNotFoundError:
        print("Error: 'Combined_SummaryTable.csv' file not found. Make sure it is in the same directory.")
        break # Exit the loop if the file is not found

    # Filter the data for the current treatment only
    df_treatment_subset = df[df['filename'] == specific_filename_for_treatment].copy()

    if df_treatment_subset.empty:
        print(f"Warning: No data found for filename '{specific_filename_for_treatment}' ({treatment_alias}). Skipping this treatment.")
        continue # Continue to the next treatment in the loop

    # Preprocessing the data
    # Rename columns according to the updated list provided
    df_treatment_subset = df_treatment_subset.rename(columns={
        'image_number': 'ds',
        'AreaShape_Area': 'y' # AreaShape_Area is chosen as the main target variable
    })

    # --- Thorough handling of the 'ds' column (image_number) ---
    df_treatment_subset['ds'] = pd.to_numeric(df_treatment_subset['ds'], errors='coerce')
    df_treatment_subset.dropna(subset=['ds'], inplace=True)
    df_treatment_subset['ds'] = np.floor(df_treatment_subset['ds']).astype(int)
    # --- End of TypeError fix ---

    # Identify identifier and shallow columns to exclude from the list of exogenous features
    identifier_cols_to_exclude = set(['ds', 'ObjectNumber', 'filename', 'Number_Object_Number'])

    # Build the list of all column names that are not identifiers
    all_features_raw_names = [col for col in df_treatment_subset.columns if col not in identifier_cols_to_exclude]

    # Filter the DataFrame columns to ensure all columns we want to use exist
    cols_to_keep_for_df_init = ['ds', 'ObjectNumber', 'filename'] + [col for col in all_features_raw_names if col in df_treatment_subset.columns]
    df_treatment_subset = df_treatment_subset[cols_to_keep_for_df_init].copy()

    # Create a unique identifier for each cell within this treatment (abbreviated treatment name + ObjectNumber)
    df_treatment_subset['unique_id'] = f"{treatment_alias}_Cell_" + df_treatment_subset['ObjectNumber'].astype(str)

    # Convert ds to datetime format for NeuralForecast (hourly frequency)
    start_date_base = datetime(2025, 1, 1) # A consistent base date for all calculations

    df_treatment_subset['ds'] = df_treatment_subset.groupby('unique_id')['ds'].transform(
        lambda x: [start_date_base + timedelta(hours=int(val)-1) for val in x]
    )
    df_treatment_subset = df_treatment_subset.explode('ds')
    df_treatment_subset['ds'] = pd.to_datetime(df_treatment_subset['ds']) # Ensure these are datetime objects

    # Data processing step for each unique_id: interpolation and ensuring continuity
    processed_data_list = []
    skipped_uids_initial = set() # Set of UIDs that are skipped at this stage
    for uid in df_treatment_subset['unique_id'].unique():
        subset_df = df_treatment_subset[df_treatment_subset['unique_id'] == uid].copy()

        subset_df = subset_df.sort_values('ds').reset_index(drop=True)

        if len(subset_df) < MIN_TOTAL_HOURS_FOR_FORECAST:
            print(f"     Warning: Cell {uid} has only {len(subset_df)} hours of data, which is less than the required minimum ({MIN_TOTAL_HOURS_FOR_FORECAST}). Skipping this cell in the following steps.")
            skipped_uids_initial.add(uid)
            continue

        full_date_range_for_uid = pd.date_range(start=subset_df['ds'].min(), periods=len(subset_df), freq='h')
        reindexed_df = subset_df.set_index('ds').reindex(full_date_range_for_uid).interpolate(method='linear')

        reindexed_df['unique_id'] = uid
        reindexed_df = reindexed_df.reset_index().rename(columns={'index': 'ds'})

        reindexed_df = reindexed_df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        if reindexed_df['y'].isnull().all():
            print(f"     Warning: The y column of cell {uid} is completely empty or NaN after processing. Skipping this cell.")
            skipped_uids_initial.add(uid)
            continue

        processed_data_list.append(reindexed_df)

    if not processed_data_list:
        print(f"No cells available for analysis in treatment {treatment_alias} after initial data filtering. Skipping this treatment.")
        continue

    mean_area_per_hour = pd.concat(processed_data_list).reset_index(drop=True)

    # Define variables for forecasting
    # We will now include the raw variables as features for the target.
    variables = [
        ('y', 'AreaShape_Area', ' Cell Area (pixels)'), # 'y' remains the primary target
        ('Acceleration', 'Acceleration', 'Acceleration'),
        ('speed_per_hour', 'Speed', 'Speed'),
        ('Distance', 'Distance', 'Distance')
    ]
    # Filter variables to include only those that actually exist in the DataFrame
    variables = [v for v in variables if v[0] in mean_area_per_hour.columns]

    if not any(v[0] == 'y' for v in variables):
        print(f"Error: The target column 'y' (AreaShape_Area) does not exist in the data for treatment {treatment_alias} after preprocessing. Cannot proceed.")
        continue

    # Get all possible exogenous features (excluding identifiers and target variables)
    base_excluded_cols_for_exog_selection = set(['ds', 'unique_id', 'y', 'filename', 'ObjectNumber', 'Number_Object_Number'])

    # Build the list of all column names that are not identifiers
    all_exog_features_candidates = [col for col in mean_area_per_hour.columns if col not in base_excluded_cols_for_exog_selection]

    # Ensure no duplicates and keep only columns that actually exist
    all_exog_features = list(set([col for col in all_exog_features_candidates if col in mean_area_per_hour.columns]))

    # --- 3. Loop over each variable for forecasting (a single LSTM model for each treatment) ---
    print("Step 3: Starting forecasting process for each variable (single LSTM model per treatment)...")

    all_forecast_results_treatment = []
    all_comparison_results_treatment = []
    all_selected_features_treatment = {}

    output_base_folder = 'forecast_results_by_treatment'
    output_folder_treatment = os.path.join(output_base_folder, treatment_alias)
    os.makedirs(output_folder_treatment, exist_ok=True)

    for var_name, var_title, y_label in variables:
        if var_name not in mean_area_per_hour.columns:
            print(f"Warning: Variable '{var_name}' does not exist in the data. Skipping forecasting for it in treatment {treatment_alias}.")
            continue

        print(f"\n--- Forecasting for: {var_title} in treatment: {treatment_alias} ---")

        cell_forecast_metrics = []
        all_selected_features_treatment[var_name] = {}

        data_for_var = mean_area_per_hour.copy()
        if var_name != 'y':
            data_for_var['y'] = data_for_var[var_name]

        uids_to_train_and_predict = []
        train_df_for_nf_list = []
        future_exog_list_for_nf_predict = []

        # --- Global feature selection for the treatment (once for all cells in the treatment) ---
        all_uids_train_data_list = []
        for uid in data_for_var['unique_id'].unique():
            if uid in skipped_uids_initial:
                continue
            uid_data_full = data_for_var[data_for_var['unique_id'] == uid].copy()
            uid_data_full = uid_data_full.sort_values('ds').reset_index(drop=True)

            if len(uid_data_full) < (HORIZON + 1):
                continue
            uid_train_df_current_cell = uid_data_full.iloc[:-HORIZON].copy()

            if len(uid_train_df_current_cell) < (MIN_TOTAL_HOURS_FOR_FORECAST - HORIZON):
                continue

            all_uids_train_data_list.append(uid_train_df_current_cell)
            uids_to_train_and_predict.append(uid)

        if not all_uids_train_data_list:
            print(f"Not enough training data for any cell in treatment {treatment_alias} for variable {var_title}. Skipping.")
            continue

        full_train_df_for_rf = pd.concat(all_uids_train_data_list).copy()

        # Filter all_exog_features to remove var_name (the current target) from exogenous features
        current_exog_features_for_this_var = [f for f in all_exog_features if f != var_name]

        # Feature selection using Random Forest on all training data for the treatment
        current_exog_features_for_rf = [f for f in current_exog_features_for_this_var if f in full_train_df_for_rf.columns]
        X_train_rf_global = full_train_df_for_rf[current_exog_features_for_rf].select_dtypes(include=[np.number]).fillna(0)
        y_train_rf_global = full_train_df_for_rf['y']

        if X_train_rf_global.empty or len(X_train_rf_global) < 2 or y_train_rf_global.empty:
            print(f"     Warning: Not enough data or numeric features for a global Random Forest in treatment {treatment_alias}. Skipping {var_title}.")
            continue

        if y_train_rf_global.isnull().all() or len(y_train_rf_global.unique()) == 1:
            print(f"     Warning: Y is constant or missing for a global Random Forest in treatment {treatment_alias}. Skipping {var_title}.")
            continue

        if X_train_rf_global.shape[1] == 0:
            print(f"     Warning: No numeric features available for a global Random Forest in treatment {treatment_alias}. Skipping {var_title}.")
            continue

        rf_global = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_global.fit(X_train_rf_global, y_train_rf_global)
        importances_global = pd.Series(rf_global.feature_importances_, index=X_train_rf_global.columns)
        top_features_global = importances_global.nlargest(10).index.tolist()

        # Assign the global selected features to each cell, so they are presented in the final output
        for uid in uids_to_train_and_predict:
            all_selected_features_treatment[var_name][uid] = top_features_global

        # --- Build the DF for NF.fit and NF.predict for all cells in the treatment ---
        for uid in uids_to_train_and_predict: # Build the lists only for those that were approved
            uid_data_full = data_for_var[data_for_var['unique_id'] == uid].copy()
            uid_data_full = uid_data_full.sort_values('ds').reset_index(drop=True)

            uid_train_df = uid_data_full.iloc[:-HORIZON].copy()
            train_df_for_nf_list.append(uid_train_df)

            uid_future_exog = uid_data_full.iloc[-HORIZON:][['ds', 'unique_id'] + top_features_global].copy() # Use the global features
            uid_future_exog = uid_future_exog.fillna(method='ffill').fillna(method='bfill').fillna(0)
            future_exog_list_for_nf_predict.append(uid_future_exog)

        # Create a single LSTM model for the entire treatment
        model = LSTM(
            h=HORIZON,
            input_size=min(LSTM_INPUT_SIZE, len(full_train_df_for_rf)), # input_size is based on all training data
            loss=MAE(),
            max_steps=100,
            scaler_type='robust',
            learning_rate=1e-3,
            num_lr_decays=3,
            futr_exog_list=top_features_global, # The globally selected exogenous features
            random_seed=42
        )
        models = [model] # List with only one model

        nf = NeuralForecast(models=models, freq='h')

        # Build the train_df for NeuralForecast with all the filtered UIDs
        required_cols_for_nf_fit = ['ds', 'unique_id', 'y'] + [f for f in top_features_global if f in train_df_for_nf_list[0].columns]
        train_df_filtered_for_nf = pd.concat([df_uid[required_cols_for_nf_fit] for df_uid in train_df_for_nf_list]).copy()

        nf.fit(df=train_df_filtered_for_nf, static_df=None, val_size=30)

        # Build the future_exog_df for NeuralForecast with all the filtered UIDs
        required_cols_for_nf_predict = ['ds', 'unique_id'] + [f for f in top_features_global if f in future_exog_list_for_nf_predict[0].columns]

        future_exog_df_for_predict = pd.concat([df_uid[required_cols_for_nf_predict] for df_uid in future_exog_list_for_nf_predict]).reset_index(drop=True)

        forecasts = nf.predict(futr_df=future_exog_df_for_predict)

        if 'unique_id' not in forecasts.columns and forecasts.index.name == 'unique_id':
            forecasts = forecasts.reset_index()

        # --- 4. Comparison, metrics, and counting good forecasts ---
        print(f"Step 4: Calculating metrics and comparisons for {var_title}...")

        for uid in uids_to_train_and_predict: # Loop only over UIDs that actually went through the forecasting process
            uid_historical_full = data_for_var[data_for_var['unique_id'] == uid].copy()
            uid_forecasts = forecasts[forecasts['unique_id'] == uid].copy()

            if uid_forecasts.empty or uid_historical_full.empty:
                continue

            uid_historical_full['ds_num'] = (uid_historical_full['ds'] - start_date_base).dt.total_seconds() / 3600 + 1
            uid_forecasts['ds_num'] = (uid_forecasts['ds'] - start_date_base).dt.total_seconds() / 3600 + 1

            uid_actual_for_comparison_df = uid_historical_full.iloc[-HORIZON:].copy()

            if len(uid_actual_for_comparison_df) != HORIZON:
                print(f"     Warning: Mismatch in the number of last historical points for {uid}. Skipping metric calculation.")
                continue

            comparison_df_uid = pd.DataFrame({
                'unique_id': uid,
                'ImageNumber_Actual': uid_actual_for_comparison_df['ds_num'].values,
                f'Forecasted_{var_title}': uid_forecasts['LSTM'].values,
                f'Actual_{var_title}': uid_actual_for_comparison_df['y'].values
            })
            all_comparison_results_treatment.append(comparison_df_uid)

            y_true = comparison_df_uid[f'Actual_{var_title}']
            y_pred = comparison_df_uid[f'Forecasted_{var_title}']

            mape = np.nan
            if not y_true.isnull().all() and len(y_true.unique()) > 1:
                if (y_true == 0).any():
                    mape = np.nan
                    print(f"     Warning: '0' value in the true data of {var_title} for cell {uid}. MAPE will be set to NaN.")
                else:
                    mape = mean_absolute_percentage_error(y_true, y_pred) * 100
            elif len(y_true.unique()) == 1 and not y_true.isnull().all():
                if y_true.iloc[0] != 0:
                    mape = np.mean(np.abs((y_true - y_pred) / y_true.iloc[0])) * 100
                else:
                    mape = np.nan

            mae = mean_absolute_error(y_true, y_pred)
            mse = mean_squared_error(y_true, y_pred)
            rmse = np.sqrt(mse)

            p_value_slope = np.nan
            try:
                # Ensure there are enough unique points to calculate a regression line
                if len(y_pred.unique()) > 1 and len(y_true.unique()) > 1:
                    slope, intercept, r_value, p_value_slope, std_err = scipy.stats.linregress(y_pred, y_true)
                else:
                    p_value_slope = np.nan
            except ValueError as e:
                print(f"     Warning: P-value calculation error for {uid} ({var_title}): {e}")
                p_value_slope = np.nan

            # The new criterion for a "good forecast": p_value_slope < 0.05
            is_good_forecast = p_value_slope < 0.05 if not np.isnan(p_value_slope) else False

            cell_forecast_metrics.append({
                'unique_id': uid,
                'variable': var_name,
                'MAE': mae,
                'MSE': mse,
                'RMSE': rmse,
                'MAPE': mape,
                'is_good_forecast': is_good_forecast,
                'p_value_slope': p_value_slope
            })

            plt.figure(figsize=(12, 6))
            plt.plot(uid_historical_full['ds_num'], uid_historical_full['y'], label=f'Historical {var_title}', marker='o', color='blue')
            plt.plot(uid_forecasts['ds_num'], uid_forecasts['LSTM'], label=f'Forecasted {var_title}', marker='x', linestyle='--', color='red')
            plt.xlabel('Time (Hours, ImageNumber)')
            plt.ylabel(y_label)
            plt.title(f'{var_title} Trend for Cell: {uid} (Treatment: {treatment_alias})\n'
                      f'MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}% (Good: {is_good_forecast})\n'
                      f'P-Value (Slope): {p_value_slope:.4f}' if not np.isnan(p_value_slope) else f'P-Value (Slope): N/A')
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(os.path.join(output_folder_treatment, f'{uid}_{var_name.lower()}_forecast_stats.png'))
            plt.close()

        if cell_forecast_metrics:
            metrics_df = pd.DataFrame(cell_forecast_metrics)

            metrics_df['Treatment_Alias'] = treatment_alias
            metrics_df['Variable_Title'] = var_title
            all_consolidated_metrics_dfs.append(metrics_df)

            good_forecast_count = metrics_df['is_good_forecast'].sum()
            total_cells_processed = len(metrics_df)
            print(f"\nSummary for {var_title} in treatment {treatment_alias}:")
            print(f"  {good_forecast_count} out of {total_cells_processed} cells with a 'good' forecast (P-Value < 0.05).")
            print(metrics_df[['unique_id', 'MAPE', 'is_good_forecast', 'p_value_slope']].head(10).to_string())
            if total_cells_processed > 10:
                print(f"... and {total_cells_processed - 10} more cells.")


    # --- 5. Finding selected features for treatment: {treatment_alias} and for each variable ---
    print(f"\nStep 5: Finding selected features for treatment: {treatment_alias} and for each variable...")

    print(f"\nSelected features common to all cells in treatment '{treatment_alias}':")
    for var_name_for_summary, var_title_for_summary, _ in variables:
        if var_name_for_summary in all_selected_features_treatment:
            sample_uid_features = next(iter(all_selected_features_treatment[var_name_for_summary].values()), [])
            if sample_uid_features:
                print(f"  For '{var_title_for_summary}': {sample_uid_features}")
            else:
                print(f"  For '{var_title_for_summary}': No selected features (the variable might have been skipped).")
        else:
            print(f"  For '{var_title_for_summary}': No selected feature data found.")


    print(f"\n--- End of forecasting and analysis process for treatment: {treatment_alias} ---")

    # Save all grouped results for the current treatment
    if all_forecast_results_treatment:
        pd.concat(all_forecast_results_treatment).to_csv(os.path.join(output_folder_treatment, 'all_cells_forecast_metrics.csv'), index=False)
    if all_comparison_results_treatment:
        pd.concat(all_comparison_results_treatment).to_csv(os.path.join(output_folder_treatment, 'all_cells_forecast_comparison.csv'), index=False)

# --- Consolidate and save all metrics to one large table at the end ---
print("\n--- Step 6: Consolidating all metrics into a global summary table ---")
if all_consolidated_metrics_dfs:
    final_global_metrics_df = pd.concat(all_consolidated_metrics_dfs).reset_index(drop=True)

    final_global_metrics_df['Cell Number'] = final_global_metrics_df['unique_id'].apply(lambda x: x.split('_')[-1])
    final_global_metrics_df['Treatment Number'] = final_global_metrics_df['Treatment_Alias'].apply(lambda x: x.split('_')[1])

    final_global_metrics_df = final_global_metrics_df.rename(columns={
        'Variable_Title': 'Feature',
        'MAPE': 'MAPE',
        'RMSE': 'RMSE',
        'MAE': 'MAE',
        'p_value_slope': 'P-Value'
    })

    final_output_columns = [
        'Cell Number',
        'Treatment Number',
        'Feature',
        'MAE',
        'RMSE',
        'MAPE',
        'P-Value',
        'is_good_forecast',
        'unique_id',
        'Treatment_Alias'
    ]
    final_global_metrics_df = final_global_metrics_df[final_output_columns]

    global_metrics_output_path = os.path.join(output_base_folder, 'all_forecast_metrics_summary_table.csv')
    final_global_metrics_df.to_csv(global_metrics_output_path, index=False, encoding='utf-8-sig')
    print(f"The global metrics summary table has been saved to: {global_metrics_output_path}")

# This code can be run in a separate cell in Colab

# Import required libraries
from google.colab import files
import shutil
import os

# Define the path of the folder you want to compress and download
folder_to_download = 'forecast_results_by_treatment'

# Ensure the folder exists
if os.path.exists(folder_to_download):
    # Compress the folder into a ZIP file
    # The name of the ZIP file will be the same as the folder name
    zip_filename = shutil.make_archive(folder_to_download, 'zip', folder_to_download)
    print(f"The folder {folder_to_download} has been compressed into file: {zip_filename}")

    # Download the ZIP file to your computer
    files.download(zip_filename)
    print(f"The file {zip_filename} has been downloaded to your computer.")
else:
    print(f"The folder '{folder_to_download}' was not found in Colab.")